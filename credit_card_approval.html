# -*- coding: utf-8 -*-
"""Credit_Card_Approval

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AyQwlsLFmyghaYax_aAz4KKBmsLcCaXG

# Loading the **Dataset and Installing Libraries**
---
"""

import kagglehub
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Download latest version
path = kagglehub.dataset_download("rikdifos/credit-card-approval-prediction")

print("Path to dataset files:", path)

application_record = pd.read_csv('/kaggle/input/credit-card-approval-prediction/application_record.csv')
application_record.head(5)

credit_record = pd.read_csv('/kaggle/input/credit-card-approval-prediction/credit_record.csv')
credit_record.head(5)

"""# **Data Cleaning**

**application_record**
"""

#validate how many rows and columns the tables have
print(application_record.shape) #(438557, 18)
print(credit_record.shape) #(1048575, 3)

# Identify duplicates
application_record.duplicated(subset=['ID']).sum() #47 id duplicados

#create a table with the duplicated data to save it in case we need to use it later.
tabla_duplicados = application_record[application_record.duplicated(subset='ID', keep=False)]
tabla_duplicados = tabla_duplicados.sort_values(by='ID')
tabla_duplicados.head()

#Duplicate values are incorrect because the same ID has different values — for example, gender.

#deletes duplicates values
application_record.drop_duplicates(subset=['ID'], inplace=True)
print(application_record.shape) #(438510, 18) without duplicate values

application_record.info()

#Review of null values
application_record.isnull().sum()

## The null values in Days_employed and Years_employed indicate that those IDs belong to unemployed individuals.

print((application_record['OCCUPATION_TYPE'].isnull().sum() / application_record.shape[0]) * 100 ,'%') # el 31% de los valores de ocupation_type son nulos (ojo: revisar la razon)

# Since it's a high percentage, i fill the null values with the word "Unknown".
application_record['OCCUPATION_TYPE'] = application_record['OCCUPATION_TYPE'].fillna('Unknown')

#Since you can't have 2.5 children, i convert the family_members column to an integer, as well as the amt_income_total column.
application_record['CNT_FAM_MEMBERS'] = application_record['CNT_FAM_MEMBERS'].astype(int)
application_record['AMT_INCOME_TOTAL'] = application_record['AMT_INCOME_TOTAL'].astype(int)

#create columns to determine the age and years of employment of each client.

# age in years
application_record['DAYS_BIRTH'] = (-application_record['DAYS_BIRTH']) // 365

# replace 365243 (unemployed) with NaN and calculate years of employment
application_record['DAYS_EMPLOYED'] = application_record['DAYS_EMPLOYED'].replace(365243, np.nan)
application_record['DAYS_EMPLOYED'] = (-application_record['DAYS_EMPLOYED']) // 365

# convert these columns to binary format
application_record['CODE_GENDER'] = application_record['CODE_GENDER'].astype(str).map({'F': 1, 'M': 0})
application_record['FLAG_OWN_CAR'] = application_record['FLAG_OWN_CAR'].astype(str).map({'Y': 1, 'N': 0})
application_record['FLAG_OWN_REALTY'] = application_record['FLAG_OWN_REALTY'].astype(str).map({'Y': 1, 'N': 0})

# convert these categorical variables to numerical values
!pip install category_encoders
import category_encoders as ce

# List of categorical variables with a logical order
categorical_cols = ['NAME_EDUCATION_TYPE']
encoder = ce.OrdinalEncoder(cols=categorical_cols) #Codifacador ordinal
application_record = encoder.fit_transform(application_record) #aplicacion al dataframe

# List of variables that do NOT have a logical order
onehot_cols = ['NAME_INCOME_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE']
encoder = ce.OneHotEncoder(cols=onehot_cols)
application_record = encoder.fit_transform(application_record)

application_record.info()

"""**Credit_Record**"""

credit_record.info()

credit_record.duplicated(subset=['STATUS']).sum()# This column contains duplicates because it represents the clients' credit history.

"""0: 1-29 days past due 1: 30-59 days past due 2: 60-89 days overdue 3: 90-119 days overdue 4: 120-149 days overdue 5: Overdue or bad debts, write-offs for more than 150 days C: paid off that month X: No loan for the month"""

# check the unique values in STATUS
print("Valores únicos en STATUS:")
print(credit_record['STATUS'].value_counts())
# Most are clients without serious delays (C, 0, X), while some have different levels of delinquency (1 to 5).

# credit_record['STATUS'] = credit_record['STATUS'].astype('category')

print(credit_record['MONTHS_BALANCE'].describe())

# The dataset covers credit information for up to 5 years.
# Most records are concentrated in the last 1 to 2 years.

credit_record.isnull().sum()

# identify delinquent clients: 1 = delinquent client, 0 = client without significant delinquency.
credit_record['STATUS'] = credit_record['STATUS'].replace({'C': '0', 'X': '0'})
credit_record['STATUS'] = credit_record['STATUS'].astype('int')
credit_record['STATUS'] = credit_record['STATUS'].apply(lambda x:1 if x >= 2 else 0)

# roup by client (ID) taking the maximum to determine if they were ever delinquent.
credit_record = credit_record.groupby('ID').agg(max).reset_index()
credit_record.drop('MONTHS_BALANCE', axis=1, inplace=True)

credit_record['STATUS'].value_counts(normalize=True)

Credit_Card_Approval = pd.merge(application_record, credit_record, on='ID', how='inner')
Credit_Card_Approval.head()

"""# **Exploratory Analysis**"""

Credit_Card_Approval.describe()

# review and clean anomalous values in `CNT_CHILDREN` since i see the highest number of children recorded is 19.
print("Distribución original CNT_CHILDREN:")
print(Credit_Card_Approval['CNT_CHILDREN'].value_counts().sort_index())

# limit the number of children to a reasonable maximum:
Credit_Card_Approval.loc[Credit_Card_Approval['CNT_CHILDREN'] > 5 , 'CNT_CHILDREN'] = 5

# review and clean anomalous values in `CNT_FAM_MEMBERS` since i see outliers starting from 7 members.
print("Distribución original CNT_FAM_MEMBERS:")
print(Credit_Card_Approval['CNT_FAM_MEMBERS'].value_counts().sort_index())

# limit the number of family members to a reasonable maximum:
Credit_Card_Approval.loc[Credit_Card_Approval['CNT_FAM_MEMBERS'] > 7, 'CNT_FAM_MEMBERS'] = 7

# Review extreme income values
plt.figure(figsize=(8, 5))
sns.boxplot(x=Credit_Card_Approval['AMT_INCOME_TOTAL'])
plt.title("Boxplot de AMT_INCOME_TOTAL (Ingresos)")
plt.show()

#Observations:
# The data shows many outliers in income, with a highly right-skewed distribution (extremely high values).
# Most of the data is concentrated at lower values, while a few extreme points reach nearly 7 million.

Credit_Card_Approval = Credit_Card_Approval[Credit_Card_Approval['AMT_INCOME_TOTAL'] < Credit_Card_Approval['AMT_INCOME_TOTAL'].quantile(0.99)]
print(f"Filas restantes después de eliminar outliers: {len(Credit_Card_Approval)}")

"""Collinearity"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.decomposition import PCA

# Create a clean copy of the DataFrame without target or ID columns
X = Credit_Card_Approval.drop(columns=['ID', 'STATUS']).copy()

# Remove or impute problematic values
X = X.replace([np.inf, -np.inf], np.nan)
X = X.fillna(X.mean(numeric_only=True))

# Final Check
print("¿Quedan NaNs?", X.isnull().values.any())
print("¿Quedan Infinitos?", np.isinf(X.values).any())

# Ensure everything is of type float (VIF requires floats, not ints)
X = X.astype(float)

# Filter only numeric predictor variables (excluding ID and STATUS)
data = Credit_Card_Approval.drop(columns=['ID', 'STATUS'])

# Correlation matrix
correlation_matrix = data.corr()
print("Matriz de correlación:")
print(correlation_matrix)

plt.figure(figsize=(18, 15))
sns.heatmap(X.corr(), cmap='coolwarm', annot=False)
plt.title("🔍 Matriz de Correlación entre variables numéricas")
plt.show()

# Calculate VIF
vif = pd.DataFrame()
vif["Variable"] = X.columns
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# show results
print("\n Factores de inflación de la varianza (VIF):")
print(vif.sort_values(by="VIF", ascending=False))

# filter VIFs
vif_sorted = vif.sort_values(by="VIF", ascending=False)

# Limit to VIF < 50 for a cleaner visualization**
vif_filtered = vif_sorted[vif_sorted["VIF"] < 50]

# Grafic
plt.figure(figsize=(14, 8))
sns.barplot(x="VIF", y="Variable", data=vif_filtered, palette="coolwarm")
plt.title("VIF aceptable)", fontsize=16)
plt.xlabel("VIF")
plt.ylabel("Variable")
plt.tight_layout()
plt.show()

# Principal Component Analysis (PCA)
pca = PCA()
pca.fit(X)

# Get the variances explained by each principal component**
explained_variance_ratio = pca.explained_variance_ratio_

print("\n Varianza explicada por cada componente principal (PCA):")
print(explained_variance_ratio)

explained_variance_ratio = pca.explained_variance_ratio_

# Cumulate Varience
cumulative_variance = np.cumsum(explained_variance_ratio)

# Bar plot: individual explained variance**
plt.figure(figsize=(12, 5))
plt.bar(range(1, len(explained_variance_ratio)+1), explained_variance_ratio, alpha=0.7, align='center')
plt.xlabel('Componente Principal')
plt.ylabel('Varianza Explicada')
plt.title('Varianza explicada por cada componente principal (PCA)')
plt.grid(True)
plt.tight_layout()
plt.show()

# Line plot: cumulative explained variance**
plt.figure(figsize=(12, 5))
plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o', linestyle='--')
plt.xlabel('Número de Componentes Principales')
plt.ylabel('Varianza Acumulada')
plt.title('Varianza explicada acumulada (PCA)')
plt.axhline(y=0.9, color='r', linestyle='--', label='90% de varianza explicada')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""**Correlation**"""

# List of redundant dummy columns to remove (one per group)**
columnas_a_eliminar = [
    'NAME_INCOME_TYPE_1',
    'NAME_FAMILY_STATUS_1',
    'NAME_HOUSING_TYPE_1',
    'OCCUPATION_TYPE_1'
]

# Remove columns from the predictor set**
X_reducido = X.drop(columns=columnas_a_eliminar)

# Ensure there are no null or infinite values**
X_reducido = X_reducido.replace([np.inf, -np.inf], np.nan)
X_reducido = X_reducido.dropna()

# Recalculate VIF
vif = pd.DataFrame()
vif["Variable"] = X_reducido.columns
vif["VIF"] = [variance_inflation_factor(X_reducido.values, i) for i in range(X_reducido.shape[1])]

# show results
print("\n VIF después de eliminar colinealidad:")
print(vif.sort_values(by="VIF", ascending=False))

# Sort VIFs from highest to lowest**
vif_sorted = vif.sort_values(by="VIF", ascending=False)

# Grafic
plt.figure(figsize=(12, 10))
sns.barplot(x="VIF", y="Variable", data=vif_sorted, palette="coolwarm")
plt.title("Factores de Inflación de la Varianza (VIF) después de eliminar colinealidad", fontsize=14)
plt.xlabel("VIF")
plt.ylabel("Variable")
plt.grid(True, axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Dimensionality reduction**
from sklearn.preprocessing import StandardScaler

# Standardize data**
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA (you can specify how many components you want)**
pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X_scaled)

print(f"Número de componentes seleccionados: {pca.n_components_}")

"""# Modelo Predictivo"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, RocCurveDisplay
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import roc_curve, auc
from imblearn.over_sampling import RandomOverSampler
from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier

# Variable separation:
X = Credit_Card_Approval.drop(columns=['ID', 'STATUS'])
y = Credit_Card_Approval['STATUS']

# Split the dataset into training and testing:
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

"""Class Balancing"""

# Check the original class imbalance
print("Distribución original:", Counter(y))

# Apply oversampling (duplicate minority class samples)
oversample = RandomOverSampler(sampling_strategy='minority', random_state=42)
X_over, y_over = oversample.fit_resample(X, y)

# Check new distribution
print("Después del oversampling:", Counter(y_over))

# Apply Undersampling
undersample = RandomUnderSampler(sampling_strategy='majority', random_state=42)
X_under, y_under = undersample.fit_resample(X, y)

print("Después del undersampling:", Counter(y_under))

"""

```
Logistic regression with PCA
```

"""

#oversampling X_pca and y

# Impute missing values before everything else
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Oversampling
ros = RandomOverSampler(sampling_strategy='minority', random_state=42)
X_resampled, y_resampled = ros.fit_resample(X_imputed, y)
print("Distribución después del Oversampling:", Counter(y_resampled))

# Scaler
scaler = StandardScaler()
X_resampled_scaled = scaler.fit_transform(X_resampled)

# PCA
pca = PCA(n_components=38, random_state=42)
X_pca_resampled = pca.fit_transform(X_resampled_scaled)

# Split en train y test
X_train, X_test, y_train, y_test = train_test_split(X_pca_resampled, y_resampled, test_size=0.3, random_state=42)

# training
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)

# Evaluation
y_pred = logreg.predict(X_test)
print("Matriz de confusión:")
print(confusion_matrix(y_test, y_pred))
print("\nReporte de clasificación:")
print(classification_report(y_test, y_pred))
print(classification_report(y_test, y_pred))

#Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# chart
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['No Aprobado', 'Aprobado'],
            yticklabels=['No Aprobado', 'Aprobado'])
plt.xlabel('Predicción')
plt.ylabel('Real')
plt.title('Matriz de Confusión - Regresión Logística (Oversampling + PCA)')
plt.tight_layout()
plt.show()

# Get positive class probabilities
y_prob = logreg.predict_proba(X_test)[:, 1]

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

#Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curva ROC - Regresión Logística (Oversampling + PCA)')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

# Undersampling
undersample = RandomUnderSampler(sampling_strategy='majority', random_state=42)
X_under, y_under = undersample.fit_resample(X, y)
print("Distribución después del Undersampling:", Counter(y_under))

# Impute missing values before everything else
imputer = SimpleImputer(strategy='mean')
X_under_imputed = imputer.fit_transform(X_under)

# scaler
scaler = StandardScaler()
X_under_scaled = scaler.fit_transform(X_under_imputed)

# PCA
pca = PCA(n_components=38, random_state=42)
X_pca_under = pca.fit_transform(X_under_scaled)

# Split
X_train, X_test, y_train, y_test = train_test_split(X_pca_under, y_under, test_size=0.3, random_state=42)

# training and evaluation
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)
y_pred = logreg.predict(X_test)

print("Matriz de confusión:")
print(confusion_matrix(y_test, y_pred))
print("\nReporte de clasificación:")
print(classification_report(y_test, y_pred))

# calculate confution matrix
cm = confusion_matrix(y_test, y_pred)

# plot
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', cbar=False,
            xticklabels=['No Aprobado', 'Aprobado'],
            yticklabels=['No Aprobado', 'Aprobado'])
plt.xlabel('Predicción')
plt.ylabel('Real')
plt.title('Matriz de Confusión - Regresión Logística (Undersampling + PCA)')
plt.tight_layout()
plt.show()

# Get positive class probabilities
y_prob = logreg.predict_proba(X_test)[:, 1]

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkred', label=f'ROC (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curva ROC - Regresión Logística (Undersampling + PCA)')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

"""RANDOM FOREST con Oversampling"""

#Datos
X = Credit_Card_Approval.drop(columns=['ID', 'STATUS'])
y = Credit_Card_Approval['STATUS']

# Oversampling
oversample = RandomOverSampler(sampling_strategy='minority', random_state=42)
X_over, y_over = oversample.fit_resample(X, y)

# Split
X_train_over, X_test_over, y_train_over, y_test_over = train_test_split(X_over, y_over, test_size=0.2, random_state=42)


print("Después del oversampling:", Counter(y_over))
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_over, y_train_over)
y_pred_rf = rf.predict(X_test_over)

# Evaluation
print("Random Forest con Oversampling:")
print(confusion_matrix(y_test_over, y_pred_rf))
print(classification_report(y_test_over, y_pred_rf))

# confution matrix
cm = confusion_matrix(y_test_over, y_pred_rf)

# plot
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['No Aprobado', 'Aprobado'],
            yticklabels=['No Aprobado', 'Aprobado'])
plt.xlabel('Predicción')
plt.ylabel('Real')
plt.title('Matriz de Confusión - Random Forest (Oversampling)')
plt.tight_layout()
plt.show()

# Get positive class probabilities
y_prob_rf = rf.predict_proba(X_test_over)[:, 1]


fpr, tpr, thresholds = roc_curve(y_test_over, y_prob_rf)
roc_auc = auc(fpr, tpr)

# plot ROC
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='green', lw=2, label=f'ROC Random Forest (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curva ROC - Random Forest con Oversampling')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

"""XGBOOST con Undersampling"""

from xgboost import XGBClassifier
from imblearn.under_sampling import RandomUnderSampler

# Undersampling
undersample = RandomUnderSampler(sampling_strategy='majority', random_state=42)
X_under, y_under = undersample.fit_resample(X, y)

print("Después del undersampling:", Counter(y_under))

# Split
X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(X_under, y_under, test_size=0.2, random_state=42)

# XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train_under, y_train_under)
y_pred_xgb = xgb.predict(X_test_under)

# Evaluation
print("XGBoost con Undersampling:")
print(confusion_matrix(y_test_under, y_pred_xgb))
print(classification_report(y_test_under, y_pred_xgb))

# Calculate confution matrix
cm = confusion_matrix(y_test_under, y_pred_xgb)

# plot
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False,
            xticklabels=['No Aprobado', 'Aprobado'],
            yticklabels=['No Aprobado', 'Aprobado'])
plt.xlabel('Predicción')
plt.ylabel('Real')
plt.title('Matriz de Confusión - XGBoost (Undersampling)')
plt.tight_layout()
plt.show()

# Get positive class probabilities
from sklearn.metrics import roc_curve, roc_auc_score
y_prob_under = xgb.predict_proba(X_test_under)[:, 1]

# ROC
fpr, tpr, _ = roc_curve(y_test_under, y_prob_under)
roc_auc = roc_auc_score(y_test_under, y_prob_under)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, label=f'XGBoost Undersampling (AUC = {roc_auc:.3f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Curva ROC - XGBoost con Undersampling')
plt.legend()
plt.grid()
plt.show()

"""XGBoost con Oversampling"""

# Oversampling
oversample = RandomOverSampler(sampling_strategy='minority', random_state=42)
X_over, y_over = oversample.fit_resample(X, y)
print("Distribución después del Oversampling:", Counter(y_over))

#  Split en train y test
X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.2, stratify=y_over, random_state=42)

# training XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)

# Preditions
y_pred = xgb.predict(X_test)
y_prob = xgb.predict_proba(X_test)[:, 1]

# Evaluations
print(" Matriz de confusión:")
print(confusion_matrix(y_test, y_pred))
print("\n Reporte de clasificación:")
print(classification_report(y_test, y_pred))

# confution matrix
cm = confusion_matrix(y_test, y_pred)

# plot
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', cbar=False,
            xticklabels=['No Aprobado', 'Aprobado'],
            yticklabels=['No Aprobado', 'Aprobado'])
plt.xlabel('Predicción')
plt.ylabel('Real')
plt.title('Matriz de Confusión - XGBoost (Oversampling)')
plt.tight_layout()
plt.show()

# clasification report
print("Reporte de clasificación:\n")
print(classification_report(y_test, y_pred, target_names=['No Aprobado', 'Aprobado']))

#ROC curve
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Get positive class probabilities
y_prob = xgb.predict_proba(X_test)[:, 1]

# ROC
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)

# plot
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, label=f'XGBoost (AUC = {roc_auc:.3f})', color='darkorange')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Curva ROC - XGBoost con Oversampling')
plt.legend()
plt.grid()
plt.show()

from xgboost import plot_importance
plot_importance(xgb,
                max_num_features=20,
                importance_type='gain',
                height=0.5)

plt.title('Importancia de características - XGBoost con Oversampling')
plt.tight_layout()
plt.show()

"""save model"""

import joblib
joblib.dump(xgb, 'modelo_xgboost_oversampling.pkl')

modelo = joblib.load('modelo_xgboost_oversampling.pkl')

"""# Graficos Extras"""

#(AGE)
plt.figure(figsize=(10,4))
sns.histplot(Credit_Card_Approval['DAYS_BIRTH'], bins=30, kde=True)
plt.title("Distribución de Edad")
plt.xlabel("Edad")
plt.show()

# YEARS_EMPLOYED
plt.figure(figsize=(10,4))
sns.histplot(Credit_Card_Approval['DAYS_EMPLOYED'], bins=30, kde=True)
plt.title("Distribución de Años Empleados")
plt.xlabel("Años Empleados")
plt.show()

Credit_Card_Approval['FLAG_EMPLOYED'] = Credit_Card_Approval['DAYS_EMPLOYED'] >= 1
Credit_Card_Approval['FLAG_EMPLOYED'] = (Credit_Card_Approval['DAYS_EMPLOYED'] >= 1).astype(int)
print(Credit_Card_Approval['FLAG_EMPLOYED'].value_counts())

plt.figure(figsize=(10, 4))
sns.histplot(Credit_Card_Approval['DAYS_EMPLOYED'], bins=30, kde=True)
plt.xlabel("Años Empleados")
plt.title("Distribución de Años Empleados (sin outliers, incluye desempleados)")
plt.show()

#pie chart
empleo_counts = Credit_Card_Approval['FLAG_EMPLOYED'].value_counts()
labels = ['Empleados', 'Desempleados']
colors = ['#66b3ff', '#ff9999']

plt.figure(figsize=(6, 6))
plt.pie(empleo_counts, labels=labels, autopct='%1.1f%%', startangle=140, colors=colors, explode=(0, 0.1))
plt.title('Proporción de Empleados vs. Desempleados')
plt.axis('equal')
plt.show()

# Bar chart for flags

flag_cols = [col for col in Credit_Card_Approval.columns if Credit_Card_Approval[col].nunique() == 2]

for col in flag_cols:
    plt.figure(figsize=(6,3))
    sns.countplot(x=Credit_Card_Approval[col])
    plt.title(f"Distribución de {col}")
    plt.show()

plt.figure(figsize=(8,5))
sns.scatterplot(x='DAYS_BIRTH', y='AMT_INCOME_TOTAL', data=Credit_Card_Approval, alpha=0.3)
sns.regplot(x='DAYS_BIRTH', y='AMT_INCOME_TOTAL', data=Credit_Card_Approval, scatter=False, color='red')
plt.title('Relación entre Edad e Ingreso Total')
plt.xlabel('Edad (años)')
plt.ylabel('Ingreso Total')
plt.grid(True)
plt.show()

#Pie chart

status_counts = Credit_Card_Approval['STATUS'].value_counts(normalize=True)

plt.figure(figsize=(6,6))
plt.pie(status_counts, labels=status_counts.index, autopct='%1.2f%%', colors=['skyblue', 'salmon'], startangle=90)
plt.title('Distribución de Morosidad (STATUS)')
plt.axis('equal')  # Círculo perfecto
plt.show()
